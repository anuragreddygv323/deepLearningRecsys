{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from platform import python_version\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "#from keras.utils import to_categorical\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Layer, Dropout, Masking\n",
    "from keras.layers import GRU, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version 2.1.6\n",
      "python version 3.6.4\n"
     ]
    }
   ],
   "source": [
    "print('keras version ' + keras.__version__)\n",
    "print('python version ' + python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_WORKING_DIR = 'c:/asi/dss/deepLearningRecsys/item_embedding_session_reco/'\n",
    "PATH_TO_ORIGINAL_DATA = '././data/'\n",
    "PATH_TO_PROCESSED_DATA = '././pre-train/'\n",
    "file_2_tokenize_name = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_tr.txt'  #original item id session train\n",
    "file_2_tokenize_name_test = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_ts.txt'   #original item id session test\n",
    "tokenized_file_name = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_tr_tk.txt'  #embedded vector session train\n",
    "tokenized_file_name_test = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_ts_tk.txt'    #embedded vector session test\n",
    "tokenized_file_name_X = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_tr_tkX.txt'      #GRU matrix input\n",
    "tokenized_file_name_Y = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_tr_tkY.txt'      #model actual prediction \n",
    "tokenized_file_name_test_X = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_ts_tkX.txt'  #GRU matrix input test\n",
    "tokenized_file_name_test_Y = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_ts_tkY.txt'\n",
    "glove_vectors_file_name = PATH_TO_PROCESSED_DATA + 'vectors_s_rcs15.txt'        #GloVe vectors\n",
    "glove_vocab_file_name = PATH_TO_PROCESSED_DATA + 'rcs15_s_vocab.txt'            #Glove vocab\n",
    "vocab_size = 14386\n",
    "sessions_train = 781486\n",
    "sessions_test = 30726 \n",
    "sequence_length = 3\n",
    "sequence_len = 3\n",
    "embedding_size = 50\n",
    "do_val = 0.2\n",
    "n_samples = 10\n",
    "hidden_neurons = 30  #RNN layer output dim\n",
    "num_iter = 20\n",
    "N = 10  #recall@\n",
    "maxW = N+3   #max item clicke we count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, nb_classes=None):\n",
    "    '''Convert class vector (integers from 0 to nb_classes)\n",
    "    to binary class matrix, for use with categorical_crossentropy.\n",
    "    '''\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(y)+1\n",
    "    Y = np.zeros((len(y), nb_classes),dtype=np.bool)\n",
    "    for i in range(len(y)):\n",
    "        idx = y[i,0]\n",
    "        Y[i, idx] = 1\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRecall(recom,actual,i):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(recom[:i])\n",
    "    result = (float) (len(act_set & pred_set)) / float(len(act_set))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate GloVe vocabulary and vectors dictionary\n",
    "def generate(glove_vectors_file_name,glove_vocab_file_name):\n",
    "    with open(glove_vocab_file_name, 'r') as f:\n",
    "        words = [x.rstrip().split(' ')[0] for x in f.readlines()]\n",
    "    with open(glove_vectors_file_name, 'r') as f:\n",
    "        vectors = {}\n",
    "        for line in f:\n",
    "            vals = line.rstrip().split(' ')\n",
    "            vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
    "\n",
    "    vocab_size = len(words)\n",
    "    vocab = {w: idx for idx, w in enumerate(words)}\n",
    "    ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "\n",
    "    vector_dim = len(vectors[ivocab[0]])\n",
    "    W = np.zeros((vocab_size, vector_dim))\n",
    "    for word, v in vectors.items():\n",
    "        if word == '<unk>':\n",
    "            continue\n",
    "        W[vocab[word], :] = v\n",
    "\n",
    "    # normalize each word vector to unit variance\n",
    "    W_norm = np.zeros(W.shape)\n",
    "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "    W_norm = (W.T / d).T\n",
    "    return (W_norm, vocab, ivocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load voacab and vectors file to dict\n",
    "os.chdir(PATH_TO_WORKING_DIR)\n",
    "W, vocab, ivocab = generate(glove_vectors_file_name, glove_vocab_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before load test\n",
      "after load test\n"
     ]
    }
   ],
   "source": [
    "#load test files\n",
    "os.chdir(PATH_TO_WORKING_DIR)\n",
    "print(\"before load test\")\n",
    "\n",
    "#load first 3 clicks embedded vectors\n",
    "XTS = np.loadtxt(tokenized_file_name_test_X).reshape((sessions_test,sequence_len,embedding_size))\n",
    "test_size = int(XTS.shape[0])\n",
    "X_test = XTS[0:test_size, :]\n",
    "\n",
    "#load next user clicks id\n",
    "with open(file_2_tokenize_name_test, 'r') as f:\n",
    "    full_data = [line.rstrip().split(' ') for line in f]\n",
    "    data = full_data\n",
    "\n",
    "print(\"after load test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 30)                7290      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14260)             442060    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14260)             0         \n",
      "=================================================================\n",
      "Total params: 449,350\n",
      "Trainable params: 449,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#load and create model\n",
    "json_string = 'm3_10_my_model_architecture.json'\n",
    "json_file = open(PATH_TO_PROCESSED_DATA + json_string, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# load weights into new model\n",
    "weights_string = 'm3_10_my_model_weights.h5'\n",
    "model.load_weights(PATH_TO_PROCESSED_DATA +weights_string)\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30726\n",
      "testing model \n",
      "recall  0.2975492938610506\n",
      "recall@ 1 0.08354878087840382\n",
      "\n",
      "recall@ 2 0.13602873942651383\n",
      "\n",
      "recall@ 3 0.1725829890202079\n",
      "\n",
      "recall@ 4 0.20154218959383785\n",
      "\n",
      "recall@ 5 0.2243835296925494\n",
      "\n",
      "recall@ 6 0.24426899456434495\n",
      "\n",
      "recall@ 7 0.2610553442666291\n",
      "\n",
      "recall@ 8 0.2742515645195771\n",
      "\n",
      "recall@ 9 0.28630451280080205\n",
      "\n",
      "recall@ 10 0.29717016197439955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use first 3 clicks to recommend\n",
    "Y_test = model.predict(X_test)\n",
    "recall = 0\n",
    "rown = 0\n",
    "rownc = 0\n",
    "recallV = [0]*N\n",
    "print(len(data))\n",
    "\n",
    "print(\"testing model \")\n",
    "for row in data:\n",
    "    if len(row) < 4:  #nothing to predict\n",
    "        rown = rown + 1\n",
    "        continue  #nothin to predict\n",
    "    \n",
    "    predicted_results = Y_test[rown]\n",
    "    dist = predicted_results\n",
    "    input_term = row[0:3]\n",
    "    for term in input_term: \n",
    "         if term in vocab:\n",
    "            index = vocab[term]\n",
    "            dist[index] = -np.Inf\n",
    "    top = np.argsort(-dist)[:N]\n",
    "    top_org = []\n",
    "    for glv_item_id in top:\n",
    "        org_item_id =ivocab[glv_item_id]\n",
    "        top_org.append(org_item_id)\n",
    "    top_org_set = set(top_org)\n",
    "    match = [click_id for click_id in row[3:maxW] if click_id in top_org_set]\n",
    "    recall = recall + (float (len(match))/ (float) (len(row[3:maxW])))\n",
    "    rownc = rownc+1\n",
    "    rown = rown + 1\n",
    "    for i in range(N):\n",
    "        recallV[i]= recallV[i] + calcRecall(top_org,row[3:maxW],(i+1))\n",
    "\n",
    "recall = (float) (recall)/(float) (rownc)\n",
    "print('recall ',recall)\n",
    "\n",
    "for i in range(N):\n",
    "    recallV[i] = (float)(recallV[i])/(float)(rownc)\n",
    "    print('recall@ '+ str(i+1) + ' ' + str(recallV[i])+\"\\n\")\n",
    "              \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict clicked items for one session\n",
    "sess_ind = 0  @session index to predict\n",
    "prediction = Y_test[sess_ind]\n",
    "top = np.argsort(-prediction)[:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:DarkMagenta;\">**???** convert top predictions from GloVe index to original index and print </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "click_ids = data[sess_ind]\n",
    "#first 3 clicks for the session\n",
    "click_ids[:???]\n",
    "#following clicks \n",
    "click_ids[???:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:DarkMagenta;\">**???** print clicked ids and find matches </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

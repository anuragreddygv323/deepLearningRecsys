{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_WORKING_DIR = 'f:/dss18/item_embedding_session_reco/'\n",
    "PATH_TO_ORIGINAL_DATA = '././data/'\n",
    "PATH_TO_PROCESSED_DATA = '././pre-train/'\n",
    "file_2_tokenize_name = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_tr.txt'  #original item id session train\n",
    "file_2_tokenize_name_test = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_ts.txt'  #original item id session test\n",
    "tokenized_file_name = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_tr_tk.txt'   #embedded vector session train\n",
    "tokenized_file_name_test = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_ts_tk.txt'   #embedded vector session test\n",
    "tokenized_file_name_X = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_tr_tkX.txt'     #GRU matrix input\n",
    "tokenized_file_name_Y = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_tr_tkY.txt'     #model actual prediction \n",
    "tokenized_file_name_test_X = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_ts_tkX.txt'  #GRU matrix input test\n",
    "tokenized_file_name_test_Y = PATH_TO_PROCESSED_DATA + 'rcs15_sesss_ts_tkY.txt'\n",
    "glove_vectors_file_name = PATH_TO_PROCESSED_DATA + 'vectors_s_rcs15.txt'      #GloVe vectors\n",
    "glove_vocab_file_name = PATH_TO_PROCESSED_DATA + 'rcs15_s_vocab.txt'          #Glove vocab\n",
    "vocab_size = 14386\n",
    "sessions_train = 781486\n",
    "sessions_test = 30726 \n",
    "sequence_length = 3\n",
    "sequence_len = 3\n",
    "embedding_size = 50\n",
    "do_val = 0.2\n",
    "n_samples = 10\n",
    "hidden_neurons = 30  #RNN layer output dim\n",
    "num_iter = 2\n",
    "N = 10  #recall@\n",
    "maxW = N+3   #max item clicke we count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Layer, Dropout, Masking\n",
    "from keras.layers import GRU, Activation\n",
    "from keras.models import Model, Sequential\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "#from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, nb_classes=None):\n",
    "    '''Convert class vector (integers from 0 to nb_classes)\n",
    "    to binary class matrix, for use with categorical_crossentropy.\n",
    "    '''\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(y)+1\n",
    "    Y = np.zeros((len(y), nb_classes),dtype=np.bool)\n",
    "    for i in range(len(y)):\n",
    "        idx = y[i,0]\n",
    "        Y[i, idx] = 1\n",
    "    return Y\n",
    "\n",
    "def generate(glove_vectors_file_name,glove_vocab_file_name):\n",
    "    with open(glove_vocab_file_name, 'r') as f:\n",
    "        words = [x.rstrip().split(' ')[0] for x in f.readlines()]\n",
    "    with open(glove_vectors_file_name, 'r') as f:\n",
    "        vectors = {}\n",
    "        for line in f:\n",
    "            vals = line.rstrip().split(' ')\n",
    "            vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
    "\n",
    "    vocab_size = len(words)\n",
    "    vocab = {w: idx for idx, w in enumerate(words)}\n",
    "    ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "\n",
    "    vector_dim = len(vectors[ivocab[0]])\n",
    "    W = np.zeros((vocab_size, vector_dim))\n",
    "    for word, v in vectors.items():\n",
    "        if word == '<unk>':\n",
    "            continue\n",
    "        W[vocab[word], :] = v\n",
    "\n",
    "    # normalize each word vector to unit variance\n",
    "    W_norm = np.zeros(W.shape)\n",
    "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "    W_norm = (W.T / d).T\n",
    "    return (W_norm, vocab, ivocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(PATH_TO_WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare experiment\n",
    "\n",
    "W, vocab, ivocab = generate(glove_vectors_file_name, glove_vocab_file_name)\n",
    "    \n",
    "print(\"loading train\")\n",
    "y1 = np.loadtxt(tokenized_file_name_Y, dtype='int').reshape((sessions_train, 1))\n",
    "Y = to_categorical(y1, len(vocab) + 1)\n",
    "X = np.loadtxt(tokenized_file_name_X).reshape((sessions_train,sequence_len,embedding_size))\n",
    "out_n = len(vocab)+1\n",
    "\n",
    "#create the model here\n",
    "print(\"Creating model...\")\n",
    "model = Sequential()\n",
    "print(\"Adding GRU ...\")\n",
    "model.add(GRU(???, input_dim=???, return_sequences=False))\n",
    "print(\"Adding dropout ...\")\n",
    "model.add(Dropout(do_val))\n",
    "print(\"adding output layer...\")\n",
    "model.add(Dense(???, input_dim=???))\n",
    "print(\"adding activation...\")\n",
    "model.add(Activation(\"softmax\"))\n",
    "print(\"compiling...\")\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
    "print(\"compiled!\")\n",
    "    \n",
    "print(model.summary())\n",
    "\n",
    "data_size = X.shape[0]\n",
    "train_size = int(data_size)\n",
    "\n",
    "X_train = X[0:train_size, :]\n",
    "Y_train = Y[0:train_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train model - will take too long time without GPU\n",
    "for II in range(1, num_iter):\n",
    "        if II < 10:\n",
    "            step = 1\n",
    "        else:\n",
    "            step = 10\n",
    "        model.fit(X_train, Y_train, batch_size=100, nb_epoch=step, validation_split=0.05)\n",
    "\n",
    "         # saving the model structures and parameters so it can be loaded later\n",
    "        z1 = \"m3_%d\" % (II)\n",
    "        json_string = model.to_json()\n",
    "        open(z1+'_model_architecture.json', 'w').write(json_string)\n",
    "        model.save_weights(z1+'_model_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
